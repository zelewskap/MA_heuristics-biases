{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieving data"
      ],
      "metadata": {
        "id": "22WMFA6jUW7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reddit"
      ],
      "metadata": {
        "id": "Lr2zN_n4UaQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw"
      ],
      "metadata": {
        "id": "GD763rIRVONQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import string"
      ],
      "metadata": {
        "id": "prBKecT4VQWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Posts"
      ],
      "metadata": {
        "id": "UPtpU39dWWcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fetch 1000 recent posts from the specified subreddit\n",
        "def fetch_posts(subreddit_name):\n",
        "    posts_data = []\n",
        "    try:\n",
        "        for submission in reddit.subreddit(subreddit_name).new(limit=1000):\n",
        "            post = {\n",
        "                'title': submission.title,\n",
        "                'score': submission.score,\n",
        "                'id': submission.id,\n",
        "                'subreddit': str(submission.subreddit),\n",
        "                'url': submission.url,\n",
        "                'num_comments': submission.num_comments,\n",
        "                'selftext': submission.selftext,\n",
        "                'created_utc': datetime.utcfromtimestamp(submission.created_utc)\n",
        "            }\n",
        "            posts_data.append(post)\n",
        "            # Sleep to avoid hitting the rate limit\n",
        "            time.sleep(1.2)  # Sleeping a bit more than 1 sec to stay within the limit\n",
        "    except praw.exceptions.APIException as e:\n",
        "        if e.error_type == \"TooManyRequests\":\n",
        "            print(\"Rate limit exceeded. Retrying in 60 seconds.\")\n",
        "            time.sleep(60)\n",
        "            return fetch_posts(subreddit_name)  # Retry\n",
        "        else:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "    return posts_data"
      ],
      "metadata": {
        "id": "D2ChefYAVhI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_excel(posts, subreddit_name, file_path):\n",
        "    if not posts:\n",
        "        print(\"No posts to save.\")\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(posts)\n",
        "\n",
        "    filename = f\"{file_path}{subreddit_name}_{datetime.now().strftime('%Y%m%d%H%M%S')}.xlsx\"\n",
        "    df.to_excel(filename, index=False)\n",
        "    print(f\"Saved {len(posts)} posts from /r/{subreddit_name} to {filename}\")"
      ],
      "metadata": {
        "id": "tIVaYb1aVl-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit = praw.Reddit(\n",
        "    client_id=client_id,\n",
        "    client_secret=client_secret,\n",
        "    user_agent=user_agent,\n",
        "    username=username,\n",
        "    password=password)"
      ],
      "metadata": {
        "id": "mcyyUiOYV1_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using subreddit name\n",
        "subreddit_name = 'aiwars'\n",
        "file_path = file_path"
      ],
      "metadata": {
        "id": "0uwqG9noWKq0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts = fetch_posts(subreddit_name)\n",
        "if posts:\n",
        "    save_to_excel(posts, subreddit_name, file_path)"
      ],
      "metadata": {
        "id": "AOQUPMK1WInO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fetch 1000 recent posts from the specified subreddit and flair\n",
        "def fetch_ai_posts(subreddit_name, flair):\n",
        "    posts_data = []\n",
        "    search_query = f'flair:\"{flair}\"'\n",
        "    try:\n",
        "        for submission in reddit.subreddit(subreddit_name).search(search_query, sort='hot', syntax='lucene', limit=1000):\n",
        "            post = {\n",
        "                'title': submission.title,\n",
        "                'score': submission.score,\n",
        "                'id': submission.id,\n",
        "                'subreddit': str(submission.subreddit),\n",
        "                'url': submission.url,\n",
        "                'num_comments': submission.num_comments,\n",
        "                'selftext': submission.selftext,\n",
        "                'created_utc': datetime.utcfromtimestamp(submission.created_utc)\n",
        "            }\n",
        "            posts_data.append(post)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "    return posts_data"
      ],
      "metadata": {
        "id": "GUVmBtg0WlsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subreddit_name = \"Futurology\"\n",
        "flair = \"AI\""
      ],
      "metadata": {
        "id": "Suyk7xQpWqks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts = fetch_ai_posts(subreddit_name, flair)\n",
        "if posts:\n",
        "    save_to_excel(posts, subreddit_name, file_path)"
      ],
      "metadata": {
        "id": "0XCNkWyOWsO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comments"
      ],
      "metadata": {
        "id": "B93QLsn3XPMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define illegal characters or use a condition to identify illegal content\n",
        "def is_legal_comment(comment_body):\n",
        "    return all(char in string.printable for char in comment_body)\n",
        "\n",
        "# Function to fetch comments based on the first post and comment ID\n",
        "def fetch_comments_from_excel(file_path, start_post_id=None, start_comment_id=None):\n",
        "    posts_df = pd.read_excel(file_path)\n",
        "    if start_post_id:\n",
        "        start_index = posts_df.index[posts_df['id'] == start_post_id].tolist()[0]\n",
        "    else:\n",
        "        start_index = 0\n",
        "\n",
        "    comments_data = []\n",
        "    batch_number = 1  # Initialize batch number\n",
        "    for index, row in enumerate(posts_df.iloc[start_index:].iterrows()):\n",
        "        _, row = row\n",
        "        post_id = row['id']\n",
        "\n",
        "        submission = reddit.submission(id=post_id)\n",
        "        submission.comments.replace_more(limit=None)\n",
        "        for comment in submission.comments.list():\n",
        "            if start_comment_id and comment.id == start_comment_id:\n",
        "                start_comment_id = None\n",
        "\n",
        "            if start_comment_id:\n",
        "                continue\n",
        "\n",
        "            if not is_legal_comment(comment.body):\n",
        "                continue\n",
        "\n",
        "            comments_data.append({\n",
        "                'post_id': post_id,\n",
        "                'comment_id': comment.id,\n",
        "                'comment_body': comment.body,\n",
        "                'comment_author': comment.author.name if comment.author else 'Deleted',\n",
        "                'comment_score': comment.score,\n",
        "                'created_utc': datetime.utcfromtimestamp(comment.created_utc)\n",
        "            })\n",
        "            time.sleep(1.2)\n",
        "\n",
        "            if len(comments_data) >= 900:\n",
        "                save_to_excel(comments_data, batch_number)\n",
        "                comments_data = []\n",
        "                batch_number += 1\n",
        "\n",
        "                time.sleep(60)\n",
        "\n",
        "    if comments_data:\n",
        "        save_to_excel(comments_data, batch_number)"
      ],
      "metadata": {
        "id": "7kUQvLtjXVHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = file_path\n",
        "\n",
        "start_post_id = '1arxh37'\n",
        "start_comment_id = 'kqni8h5'\n",
        "fetch_comments_from_excel(file_path, start_post_id, start_comment_id)"
      ],
      "metadata": {
        "id": "fiBviBqSX6pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## X"
      ],
      "metadata": {
        "id": "PKkvPZmfUbwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "from tweepy import Client"
      ],
      "metadata": {
        "id": "GfQiL2IrYAG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = tweepy.Client(\n",
        "    bearer_token=bearer_token,\n",
        "    consumer_key=consumer_key,\n",
        "    consumer_secret=consumer_secret,\n",
        "    access_token=access_token,\n",
        "    access_token_secret=access_token_secret)"
      ],
      "metadata": {
        "id": "5czyNxMYWuWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Posts"
      ],
      "metadata": {
        "id": "mYyK6iKVYONz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fetch the posts\n",
        "def fetch_tweets(query, max_results=100):\n",
        "\n",
        "    query += \" lang:en -is:retweet\"\n",
        "    tweet_fields = [\"attachments\", \"author_id\", \"conversation_id\", \"created_at\", \"id\", \"in_reply_to_user_id\", \"text\"]\n",
        "    tweets_list = []\n",
        "\n",
        "    try:\n",
        "        for tweet in tweepy.Paginator(client.search_recent_tweets, query=query,\n",
        "                                      tweet_fields=','.join(tweet_fields),\n",
        "                                      sort_order=\"relevancy\",\n",
        "                                      max_results=min(max_results, 100)).flatten(limit=max_results):\n",
        "            tweets_list.append(tweet)\n",
        "\n",
        "            # Sleep to respect rate limit (60 requests per 15 mins -> 1 request per 15 seconds)\n",
        "            time.sleep(15)\n",
        "\n",
        "    except tweepy.TweepyException as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return tweets_list"
      ],
      "metadata": {
        "id": "jXTiWWkjYPfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save the retrieved posts\n",
        "def save_tweets_to_excel(tweets, file_path, search_phrase):\n",
        "    if tweets:\n",
        "        tweets_data = [{\n",
        "            \"Conversation ID\": str(tweet.conversation_id),\n",
        "            \"Tweet ID\": str(tweet.id),\n",
        "            \"Author ID\": str(tweet.author_id),\n",
        "            \"Created At\": tweet.created_at.strftime('%Y-%m-%d %H:%M:%S') if tweet.created_at else 'N/A',\n",
        "            \"Text\": tweet.text,\n",
        "            \"In reply to\": str(tweet.in_reply_to_user_id)\n",
        "\n",
        "        } for tweet in tweets]\n",
        "\n",
        "\n",
        "        tweets_df = pd.DataFrame(tweets_data)\n",
        "\n",
        "        timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "        sanitized_search_phrase = ''.join(char for char in search_phrase if char.isalnum() or char in \"._-\")\n",
        "        filename = f\"{file_path}/Tweets_{sanitized_search_phrase}_{timestamp}.xlsx\"\n",
        "        filename = ''.join(char for char in filename if char.isalnum() or char in \"._-/\\\\ \").rstrip()\n",
        "\n",
        "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
        "            tweets_df.to_excel(writer, sheet_name='Tweets', index=False)\n",
        "\n",
        "        return tweets_df\n",
        "\n",
        "        print(f\"Saved {len(tweets)} tweets to {filename}\")\n",
        "    else:\n",
        "        print(\"No tweets to save.\")"
      ],
      "metadata": {
        "id": "MgeArjkPYXv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"artificial intelligence\"\n",
        "file_path = file_path\n",
        "\n",
        "tweets = fetch_tweets(query)\n",
        "save_tweets_to_excel(tweets, file_path, query)"
      ],
      "metadata": {
        "id": "KREiwJGUYkan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comments"
      ],
      "metadata": {
        "id": "9F-ELFTaYt4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function reads an Excel file for Conversation IDs, fetches up to 1000 comments in total for these IDs\n",
        "def fetch_comments(input_file_path, output_file_path):\n",
        "\n",
        "    df = pd.read_excel(input_file_path)\n",
        "    all_comments = []\n",
        "    max_comments = 900\n",
        "\n",
        "    for conversation_id in df['Conversation ID'].iloc[7:]:\n",
        "        if len(all_comments) < max_comments:\n",
        "\n",
        "            query = f'conversation_id:{conversation_id}'\n",
        "            try:\n",
        "                for tweet in tweepy.Paginator(client.search_recent_tweets, query=query,\n",
        "                                              tweet_fields=[\"conversation_id\", \"author_id\", \"created_at\", \"id\", \"text\", \"in_reply_to_user_id\"],\n",
        "                                              max_results=100).flatten():\n",
        "                    if len(all_comments) >= max_comments:\n",
        "                        break\n",
        "                    all_comments.append(tweet)\n",
        "\n",
        "                time.sleep(12)\n",
        "\n",
        "            except tweepy.TweepyException as e:\n",
        "                print(f\"An error occurred: {e}\")\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "\n",
        "\n",
        "    if all_comments:\n",
        "        save_tweets_to_excel(all_comments, output_file_path, \"comments_climatechange_3\")\n",
        "\n",
        "    else:\n",
        "        print(\"No comments were fetched.\")"
      ],
      "metadata": {
        "id": "Ji664vH2YwHu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = input_file_path\n",
        "output_file_path = output_file_path"
      ],
      "metadata": {
        "id": "37x4l7lXY_La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fetch_comments(input_file_path, output_file_path)"
      ],
      "metadata": {
        "id": "kfwFagIhZDIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TikTok"
      ],
      "metadata": {
        "id": "0H13kR0qUcuv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-JLWKrOTQ0X"
      },
      "outputs": [],
      "source": [
        "!pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import xlsxwriter\n",
        "from datetime import datetime, timedelta"
      ],
      "metadata": {
        "id": "3hWIku-5ZIk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  Generating Client Access Token\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "81ffSMdkZOD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Videos"
      ],
      "metadata": {
        "id": "wLwiy0o3Zosn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function fetches videos on TikTok based on the criteria\n",
        "def fetch_videos(search_phrase1, search_phrase2, access_token, max_videos=500):\n",
        "    url = 'https://open.tiktokapis.com/v2/research/video/query/'\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {access_token}',\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    query_params = {\n",
        "      \"fields\": \"id,video_description,create_time,region_code,share_count,view_count,like_count,comment_count,hashtag_names,username,voice_to_text\"\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"query\": {\n",
        "        \"and\":[\n",
        "          {\n",
        "            \"operation\":\"IN\",\n",
        "            \"field_name\":\"region_code\",\n",
        "            \"field_values\":[\"US\"]\n",
        "          },\n",
        "          {\n",
        "            \"operation\": \"EQ\",\n",
        "            \"field_name\": \"hashtag_name\",\n",
        "            \"field_values\": [search_phrase1]\n",
        "\n",
        "          },\n",
        "          {\n",
        "            \"operation\": \"EQ\",\n",
        "            \"field_name\": \"hashtag_name\",\n",
        "            \"field_values\": [search_phrase2]\n",
        "\n",
        "          }\n",
        "        ]\n",
        "        },\n",
        "        \"start_date\": \"20240101\",\n",
        "        \"end_date\": \"20240130\",\n",
        "        \"max_count\": 100,\n",
        "    }\n",
        "\n",
        "    videos = []\n",
        "    cursor = None\n",
        "\n",
        "    while len(videos) < max_videos:\n",
        "        if cursor:\n",
        "            payload['cursor'] = cursor\n",
        "\n",
        "        response = requests.post(url, headers=headers, params=query_params, json=payload)\n",
        "        if response.status_code == 200:\n",
        "            response_json = response.json()\n",
        "            video_data = response_json.get('data', {}).get('videos', [])\n",
        "            cursor = response_json.get('data', {}).get('next_cursor')\n",
        "\n",
        "            for video in video_data:\n",
        "                if isinstance(video, dict):\n",
        "                    video_id = video.get('id')\n",
        "                    username = video.get('username', 'unknown')\n",
        "                    video_url = f\"https://www.tiktok.com/@{username}/video/{video_id}\"\n",
        "                    video['Video URL'] = video_url\n",
        "                    videos.append(video)\n",
        "\n",
        "            if not video_data or not cursor:\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Failed to fetch videos: {response.text}\")\n",
        "            break\n",
        "\n",
        "        if len(videos) >= max_videos:\n",
        "            videos = videos[:max_videos]\n",
        "\n",
        "    return videos"
      ],
      "metadata": {
        "id": "7Dzdm6QfZqrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_excel(videos, file_path):\n",
        "  if videos:\n",
        "    videos_df = pd.DataFrame(videos)\n",
        "\n",
        "    filename = f\"{file_path}/TikTok_Videos_{search_phrase1}_{datetime.now().strftime('%Y%m%d%H%M%S')}.xlsx\"\n",
        "\n",
        "    with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
        "        videos_df.to_excel(writer, sheet_name='Videos', index=False)\n",
        "\n",
        "    print(f\"Saved {len(videos)} videos to {filename}\")"
      ],
      "metadata": {
        "id": "hKku9FqdZ8ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_phrase1 = \"ai\"\n",
        "search_phrase2 = \"conspiracy\""
      ],
      "metadata": {
        "id": "zahD_OvjZ9_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = file_path\n",
        "\n",
        "videos = fetch_videos(search_phrase1, search_phrase2, access_token)\n",
        "\n",
        "save_to_excel(videos, file_path)"
      ],
      "metadata": {
        "id": "fexl6VazaAzL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}